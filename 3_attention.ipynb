{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mini_torch import Tensor, nn, optim, F, schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11469a",
   "metadata": {},
   "source": [
    "## Create Synthetic Text Dataset\n",
    "\n",
    "We'll create a simple synthetic dataset for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple synthetic text classification dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 1000  # Vocabulary size\n",
    "max_seq_len = 50   # Maximum sequence length\n",
    "embed_dim = 64     # Embedding dimension\n",
    "n_classes = 2      # Binary classification (positive/negative sentiment)\n",
    "\n",
    "# Generate synthetic data\n",
    "# Positive samples: contain more tokens from upper half of vocabulary\n",
    "# Negative samples: contain more tokens from lower half of vocabulary\n",
    "\n",
    "def generate_sequence(is_positive, seq_len=50):\n",
    "    \"\"\"Generate a synthetic sequence\"\"\"\n",
    "    if is_positive:\n",
    "        # Positive: more tokens from upper half (500-1000)\n",
    "        tokens = np.random.choice(range(500, vocab_size), size=int(seq_len * 0.7))\n",
    "        noise = np.random.choice(range(vocab_size), size=int(seq_len * 0.3))\n",
    "    else:\n",
    "        # Negative: more tokens from lower half (0-500)\n",
    "        tokens = np.random.choice(range(0, 500), size=int(seq_len * 0.7))\n",
    "        noise = np.random.choice(range(vocab_size), size=int(seq_len * 0.3))\n",
    "    \n",
    "    seq = np.concatenate([tokens, noise])\n",
    "    np.random.shuffle(seq)\n",
    "    return seq[:seq_len]\n",
    "\n",
    "# Generate training data (2000 samples)\n",
    "n_train = 2000\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_train // 2):\n",
    "    # Positive samples\n",
    "    X_train.append(generate_sequence(is_positive=True, seq_len=max_seq_len))\n",
    "    y_train.append(1)\n",
    "    \n",
    "    # Negative samples\n",
    "    X_train.append(generate_sequence(is_positive=False, seq_len=max_seq_len))\n",
    "    y_train.append(0)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Generate test data (500 samples)\n",
    "n_test = 500\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(n_test // 2):\n",
    "    X_test.append(generate_sequence(is_positive=True, seq_len=max_seq_len))\n",
    "    y_test.append(1)\n",
    "    X_test.append(generate_sequence(is_positive=False, seq_len=max_seq_len))\n",
    "    y_test.append(0)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Shuffle\n",
    "train_idx = np.random.permutation(len(X_train))\n",
    "X_train = X_train[train_idx]\n",
    "y_train = y_train[train_idx]\n",
    "\n",
    "test_idx = np.random.permutation(len(X_test))\n",
    "X_test = X_test[test_idx]\n",
    "y_test = y_test[test_idx]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "print(f\"\\nSample sequence (first 10 tokens): {X_train[0][:10]}\")\n",
    "print(f\"Sample label: {y_train[0]} ({'Positive' if y_train[0] == 1 else 'Negative'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775acd34",
   "metadata": {},
   "source": [
    "## Define Attention-Based Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0caf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based sequence classifier.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer\n",
    "    2. Positional encoding (simple learned)\n",
    "    3. Multi-head attention layers\n",
    "    4. Global average pooling\n",
    "    5. Classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, \n",
    "                 max_seq_len, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layer (simplified - using random embeddings)\n",
    "        # In practice, this would be learned\n",
    "        self.embedding = Tensor(np.random.randn(vocab_size, embed_dim).astype(np.float32) * 0.01)\n",
    "        \n",
    "        # Positional encoding (learned)\n",
    "        self.pos_encoding = Tensor(np.random.randn(max_seq_len, embed_dim).astype(np.float32) * 0.01)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = []\n",
    "        for _ in range(num_layers):\n",
    "            block = nn.TransformerBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=embed_dim * 4,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            self.transformer_blocks.append(block)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Collect parameters\n",
    "        self._parameters = [self.embedding, self.pos_encoding]\n",
    "        for block in self.transformer_blocks:\n",
    "            self._parameters.extend(block.parameters())\n",
    "        self._parameters.extend(self.classifier.parameters())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token indices, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embedding lookup: (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        # Simple implementation: gather embeddings\n",
    "        embedded = []\n",
    "        for b in range(batch_size):\n",
    "            seq_emb = []\n",
    "            for t in range(seq_len):\n",
    "                token_idx = int(x.data[b, t])\n",
    "                seq_emb.append(self.embedding.data[token_idx])\n",
    "            embedded.append(np.stack(seq_emb))\n",
    "        embedded = Tensor(np.stack(embedded))  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_enc = self.pos_encoding[:seq_len].unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "        x = embedded + pos_enc  # Broadcasting over batch\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Global average pooling over sequence dimension\n",
    "        x = x.mean(axis=1)  # (batch, embed_dim)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier.modules_list[0](x)  # First linear\n",
    "        logits = logits.relu()\n",
    "        logits = self.classifier.modules_list[1](logits)  # Dropout\n",
    "        logits = self.classifier.modules_list[2](logits)  # Final linear\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "print(\"Creating Attention-based Classifier...\")\n",
    "model = AttentionClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_classes=n_classes,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model created with {len(model.parameters())} parameter tensors\")\n",
    "print(f\"Embedding dimension: {embed_dim}\")\n",
    "print(f\"Number of attention heads: 4\")\n",
    "print(f\"Number of transformer layers: 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaef153",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, X_train, y_train, optimizer, batch_size=32):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    n_samples = X_train.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        # Get batch\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Convert to Tensors\n",
    "        X_tensor = Tensor(X_batch.astype(np.float32))\n",
    "        y_tensor = Tensor(y_batch.astype(np.float32), requires_grad=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_tensor)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, y_tensor)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    n_samples = X_test.shape[0]\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        X_batch = X_test[i:i+batch_size]\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        X_tensor = Tensor(X_batch.astype(np.float32))\n",
    "        y_tensor = Tensor(y_batch.astype(np.float32), requires_grad=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_tensor)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, y_tensor)\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = np.argmax(logits.data, axis=1)\n",
    "        \n",
    "        correct += (predictions == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "    \n",
    "    return correct / total, total_loss / n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df183f",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = schedulers.LRSchedulerOnPlateau(\n",
    "    optimizer, \n",
    "    initial_lr=learning_rate, \n",
    "    patience=3, \n",
    "    factor=0.5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Attention Model Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, X_train, y_train, optimizer, batch_size)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_acc, test_loss = evaluate(model, X_test, y_test, batch_size)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.learning_rate:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feecaa",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, marker='o', label='Train Loss', linewidth=2)\n",
    "ax1.plot(test_losses, marker='s', label='Test Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(test_accuracies, marker='o', color='green', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label='Random Baseline', alpha=0.5)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Test Accuracy: {max(test_accuracies):.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35c9a2",
   "metadata": {},
   "source": [
    "## Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "X_tensor = Tensor(X_test[:100].astype(np.float32))\n",
    "logits = model(X_tensor)\n",
    "predictions = np.argmax(logits.data, axis=1)\n",
    "true_labels = y_test[:100]\n",
    "\n",
    "# Analyze predictions\n",
    "correct = predictions == true_labels\n",
    "accuracy = correct.sum() / len(correct)\n",
    "\n",
    "print(\"Prediction Analysis (first 100 samples):\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Correct: {correct.sum()}/{len(correct)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'True Label':<15} {'Predicted':<15} {'Confidence':<15} {'Correct'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(20):\n",
    "    true_label = 'Positive' if true_labels[i] == 1 else 'Negative'\n",
    "    pred_label = 'Positive' if predictions[i] == 1 else 'Negative'\n",
    "    \n",
    "    # Get confidence (softmax probability)\n",
    "    probs = F.softmax(Tensor(logits.data[i:i+1]), axis=1)\n",
    "    confidence = probs.data[0, predictions[i]]\n",
    "    \n",
    "    is_correct = '✓' if correct[i] else '✗'\n",
    "    \n",
    "    print(f\"{true_label:<15} {pred_label:<15} {confidence:<15.3f} {is_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c5184",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3638a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get all predictions\n",
    "all_predictions = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i+batch_size]\n",
    "    X_tensor = Tensor(X_batch.astype(np.float32))\n",
    "    logits = model(X_tensor)\n",
    "    preds = np.argmax(logits.data, axis=1)\n",
    "    all_predictions.extend(preds)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Accuracy:  {(tp + tn) / (tp + tn + fp + fn):.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e48a5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Multi-Head Attention** implementation from scratch\n",
    "2. **Transformer blocks** with attention and feed-forward networks\n",
    "3. **Sequence classification** using attention mechanisms\n",
    "4. **Positional encoding** for sequence ordering\n",
    "5. Training with **Adam optimizer** and **learning rate scheduling**\n",
    "\n",
    "The attention mechanism allows the model to focus on different parts of the input sequence when making predictions, which is crucial for understanding context in sequential data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
